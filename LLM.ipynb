{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7586077-6692-4593-b93c-cc45d22cd141",
   "metadata": {},
   "source": [
    "# Building a GPT from scratch - pre-training, fine-tuning, RAGs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532336db-4b0f-429e-95c2-f9e35625fc96",
   "metadata": {},
   "source": [
    "# Source file is available at https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "01a02659-f600-4b06-a42f-0db4d9ebb394",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "399cf88a-1e06-43d8-b543-e3be5678a89e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters = 1115394 \n",
      " ************************ \n",
      " First 100 characters are: \n",
      " ************************ \n",
      " First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    shakespeare_text = f.read()\n",
    "print(f'Number of characters = {len(text)} \\n ************************ \\n First 100 characters are: \\n ************************ \\n {text[:100]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6f0a62-2a52-49a4-af5b-3474da3a496b",
   "metadata": {},
   "source": [
    "# We will build a simple transformer on character level vocabulary. We will do the following:\n",
    "1) build a small vocabulary,\n",
    "2) test an example,\n",
    "3) encode our text,\n",
    "4) split it into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "322eb7ed-83c9-47f1-b4b4-82a6eb63086b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz \n",
      "vocab size =  65\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(list(set(text)))\n",
    "vocab_size = len(vocab)\n",
    "print(''.join(vocab), \"\\nvocab size = \", vocab_size)\n",
    "encode_mapping = {ch:i for i, ch in enumerate(vocab)}\n",
    "decode_mapping = {i:ch for i, ch in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "cf28f931-5c5d-4094-860d-d2e67d6e9f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text):\n",
    "    encode_lambda = lambda s: [encode_mapping[ch] for ch in s]\n",
    "    return torch.tensor(encode_lambda(text))\n",
    "\n",
    "def decode(coded_sentence_tensor):\n",
    "    decode_lambda = lambda s: ''.join([decode_mapping[ch] for ch in s])\n",
    "    return decode_lambda(coded_sentence_tensor.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "29a58dc0-2df5-418c-b326-ec48c55c668e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([46, 47, 47,  1, 58, 46, 43, 56, 43])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = encode('hii there')\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "de3366bf-6f22-47c9-a316-7b63b2ba00f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hii there'"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "9f898aed-1e3e-4923-a8af-941c70180c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(data) = 1115394\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59]) First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "### Code the shakespeare text ###\n",
    "\n",
    "data = encode(shakespeare_text)\n",
    "print(f\"len(data) = {len(data)}\")\n",
    "print(data[:100], shakespeare_text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "a806bc4a-917e-42e5-9c60-52861d6e1411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size = 1003854, and test size = 111540\n"
     ]
    }
   ],
   "source": [
    "### Split data into train and test ###\n",
    "\n",
    "train = data[:int(0.9*len(data))]\n",
    "test = data[int(0.9*len(data)):]\n",
    "print(f\"train size = {len(train)}, and test size = {len(test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31d0ee7-8720-451e-8fa9-2a4991292931",
   "metadata": {},
   "source": [
    "# For a transformer, we need to split the data into batches, where each back contains a block of numbers. \n",
    "## Block size = 8. \n",
    "## batch size = number of blocks that will be processed in parallel\n",
    "\n",
    "Each block of 8 consists of 8 training examples for the transformer. Let's see how with an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "a707fccf-7ec6-42b3-923a-588407c8b718",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 8\n",
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "50b24c1f-00e3-45e9-a804-f300b3c21c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when training on tensor([18]), target is 47\n",
      "when training on tensor([18, 47]), target is 56\n",
      "when training on tensor([18, 47, 56]), target is 57\n",
      "when training on tensor([18, 47, 56, 57]), target is 58\n",
      "when training on tensor([18, 47, 56, 57, 58]), target is 1\n",
      "when training on tensor([18, 47, 56, 57, 58,  1]), target is 15\n",
      "when training on tensor([18, 47, 56, 57, 58,  1, 15]), target is 47\n",
      "when training on tensor([18, 47, 56, 57, 58,  1, 15, 47]), target is 58\n"
     ]
    }
   ],
   "source": [
    "x = data[:block_size]\n",
    "y = data[1:block_size+1]\n",
    "\n",
    "for index in range(block_size):\n",
    "    print(f\"when training on {x[:index+1]}, target is {y[index]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "f6e7a8d3-3f31-47be-a36f-0f86157f8561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs =  tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "outputs =  tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "Input shape = torch.Size([4, 8])\n",
      "Output shape = torch.Size([4, 8])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "def get_batch(split):\n",
    "    data = train if split == \"train\" else test\n",
    "    random_indexes = torch.randint(len(data)-block_size, (batch_size, ))\n",
    "    x = torch.stack([data[i:i+block_size] for i in random_indexes])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in random_indexes])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print(\"inputs = \", xb)\n",
    "print(\"outputs = \", yb)\n",
    "print(f\"Input shape = {xb.shape}\")\n",
    "print(f\"Output shape = {yb.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "4ab45e01-f614-4552-9b6a-27a6f5689b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When input = [24], target = 43\n",
      "When input = [24, 43], target = 58\n",
      "When input = [24, 43, 58], target = 5\n",
      "When input = [24, 43, 58, 5], target = 57\n",
      "When input = [24, 43, 58, 5, 57], target = 1\n",
      "When input = [24, 43, 58, 5, 57, 1], target = 46\n",
      "When input = [24, 43, 58, 5, 57, 1, 46], target = 43\n",
      "When input = [24, 43, 58, 5, 57, 1, 46, 43], target = 39\n",
      "When input = [44], target = 53\n",
      "When input = [44, 53], target = 56\n",
      "When input = [44, 53, 56], target = 1\n",
      "When input = [44, 53, 56, 1], target = 58\n",
      "When input = [44, 53, 56, 1, 58], target = 46\n",
      "When input = [44, 53, 56, 1, 58, 46], target = 39\n",
      "When input = [44, 53, 56, 1, 58, 46, 39], target = 58\n",
      "When input = [44, 53, 56, 1, 58, 46, 39, 58], target = 1\n",
      "When input = [52], target = 58\n",
      "When input = [52, 58], target = 1\n",
      "When input = [52, 58, 1], target = 58\n",
      "When input = [52, 58, 1, 58], target = 46\n",
      "When input = [52, 58, 1, 58, 46], target = 39\n",
      "When input = [52, 58, 1, 58, 46, 39], target = 58\n",
      "When input = [52, 58, 1, 58, 46, 39, 58], target = 1\n",
      "When input = [52, 58, 1, 58, 46, 39, 58, 1], target = 46\n",
      "When input = [25], target = 17\n",
      "When input = [25, 17], target = 27\n",
      "When input = [25, 17, 27], target = 10\n",
      "When input = [25, 17, 27, 10], target = 0\n",
      "When input = [25, 17, 27, 10, 0], target = 21\n",
      "When input = [25, 17, 27, 10, 0, 21], target = 1\n",
      "When input = [25, 17, 27, 10, 0, 21, 1], target = 54\n",
      "When input = [25, 17, 27, 10, 0, 21, 1, 54], target = 39\n"
     ]
    }
   ],
   "source": [
    "for i in range(batch_size):\n",
    "    for j in range(block_size):\n",
    "        context = xb[i, :j+1]\n",
    "        target = yb[i, j]\n",
    "        print(f\"When input = {context.tolist()}, target = {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ce0b77-f249-4dc9-a93c-ebba48a87f62",
   "metadata": {},
   "source": [
    "# We first get a stupid model where the tokens in a block do not talk to each other. The next character prediction is based out of a look up table, not from attention mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "8233e675-af94-4bf5-86a9-ef31a6b9310a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10bd218b0>"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "b8cb943d-c6f3-48ba-a395-e77ed0d911ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Class structure taken from https://pytorch.org/docs/stable/generated/torch.nn.Module.html\n",
    "\n",
    "# We need to \n",
    "# 1) Create a forward pass, i.e., define an embedding table, a single node network that simply looks up the embedding of the previous token, gets the value, creates the logit, and gets loss function due to logit (since we know the actual output). \n",
    "# 2) Create a decoder pass that generates the next output using the forward pass\n",
    "# Note that we don't need to create the logit and the loss function. We do this so that when we improve the model, we can compute the loss and see it drop.\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, target=None):\n",
    "        # idx is the current context\n",
    "        # target is the next predicted character\n",
    "        # Due to parallel processing, idx and targets are both (batch_size, block_size) tensor of integers\n",
    "        B, T = idx.shape\n",
    "        logits = self.token_embedding_table(idx) # logits is of size (batch_size, block_size, vocab_size) = (B, T, C)\n",
    "        \n",
    "        if target == None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C) # 32 x 65\n",
    "            target = target.view(B*T)\n",
    "            loss = F.cross_entropy(logits, target) \n",
    "            # Pytorch will automatically change target to (B*T, 1) and replicate it C times across that dimension to compute loss\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is the current context\n",
    "        # max_new_tokens is the max number of characters to generate\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, loss = self.forward(idx)\n",
    "            relevant_logits = logits[:, -1, :] # we get the last time step in the sequence: becomes (B, 1, C).\n",
    "            probs = F.softmax(relevant_logits, dim = -1) # Becomes (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples = 1)\n",
    "            idx = torch.cat((idx, idx_next), dim = 1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "758cea40-1e02-4713-b825-d43e254e8a74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
       "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
       "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
       "        [17, 27, 10,  0, 21,  1, 54, 39]])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "cd4b7992-f5aa-4fb9-aa19-eb79209a8e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1]) torch.Size([4, 8]) torch.Size([4, 8])\n",
      "torch.Size([32, 65]) tensor(4.8786, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "m = BigramLanguageModel()\n",
    "idx = torch.zeros((1, 1), dtype=torch.long)\n",
    "print(idx.shape, xb.shape, yb.shape)\n",
    "# logits, loss = m.forward(idx)\n",
    "logits, loss = m.forward(xb, yb)\n",
    "print(logits.shape, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "f94902bf-c8ae-4c57-8968-7ffc3a709dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CCA't-L!BotNX CJ?.yZBCbYiKH;P YkRocBUMykIfFGRetY!uHN.cp$kzo,I&fiMD-rbjJmho,Rpw:vZEQvjKHAUzoTOmw$I.KX\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(idx = torch.zeros((1,1), dtype=torch.long), max_new_tokens=100)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535a8b77-d4cc-48a0-8249-ae51e25b4425",
   "metadata": {},
   "source": [
    "### Let's optimize now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "04932aac-80ef-492e-b1dc-4d8feac4bfd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.772301197052002\n",
      "\n",
      "DpJG-gYQThXtN3XNAc;KMbObHRlylTyuF-d,';LGjF&sr'Rjj?jK$?Y!$dD.i\n",
      "SaQJpOlytJ;'thtriVc'AeDnYUXeawa!uvqHWcsV.uDzHQH:CVJxrhsgUv&y\n",
      "GbOJQEXotow:'uM qfT'aI-apDqIVlHnUQYsBJ$wLgnqfI!qKoi f'sBN:WKjKBylfRqPN&QTVqr&y'rLftimb\n",
      "XXgCCnpM:CjRvvV&bC-FtoisyuQ&fSVwUApxEsTVrcMAEZgXXlxitoCU'PwhrIfkId?OEo.RdKo?nQisrdnuGNpx$pZGRr&yyh$-IPy'Ih -Yu$G.pJMOE!q?Z3evX-KAk.IBT!mVjnm\n",
      "qMDMphYya!UNXAT:CH-bk$Mp$DV,,e:COs&vEsV3m,uybPd?pUGRlx\n",
      "pxyTsStepkgf?IiHTbR\n",
      "V?uj?Y3i.aUiGGviClI!U;A-QQW:sk!lj$ Plg\n",
      "wIFERw fDAAJO'h;\n",
      ",phsVHxBo.\n",
      "3n;pJiG\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n",
    "batch_size = 32\n",
    "for steps in range(100): # increase number of steps for good results...\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())\n",
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffb8fcb-2b09-42cd-8f4b-87a34bc36f57",
   "metadata": {},
   "source": [
    "# The above result is quite garbage. Now, let's add attention block so the tokens can communicate with each other"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f41894-7b80-4056-8290-7b8df360cb3d",
   "metadata": {},
   "source": [
    "# Create one head of self attention\n",
    "We will make one change. Instead of each batch, block of size vocab_size, we are going to use an embedding of size n_embed on top of the simple embedding of vocab_size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "e8a0c414-e1a0-4442-8117-55d27da30ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layer = 4 # Number of transformer blocks to put in sequence\n",
    "n_head = 4 # number of multi heads in each transformer block\n",
    "\n",
    "# Parameters for each head are as follows:\n",
    "batch_size = 16\n",
    "block_size = 32\n",
    "n_embed = 64\n",
    "learning_rate = 3e-4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "eval_iters = 200\n",
    "\n",
    "dropout = 0.0\n",
    "head_size = n_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "6bac51c4-4c9e-4a98-8f3a-e946eea6aca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layer = 6 # Number of transformer blocks to put in sequence\n",
    "n_head = 6 # number of multi heads in each transformer block\n",
    "\n",
    "# Parameters for each head are as follows:\n",
    "batch_size = 64\n",
    "block_size = 256\n",
    "n_embed = 384 # 64*6 = batch_size * n_head\n",
    "learning_rate = 3e-4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "max_iters = 500\n",
    "eval_interval = 10\n",
    "eval_iters = 20\n",
    "\n",
    "dropout = 0.2\n",
    "head_size = n_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "06194ac4-3ca3-45cd-9380-bce61305cad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train if split == 'train' else test\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'test']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.query = nn.Linear(n_embed, head_size, bias = False)\n",
    "        self.key = nn.Linear(n_embed, head_size, bias = False)\n",
    "        self.value = nn.Linear(n_embed, head_size, bias = False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, idx):\n",
    "        # idx is the current context\n",
    "        # target is the next predicted character\n",
    "        # Due to parallel processing, idx and targets are both (batch_size, block_size) tensor of integers\n",
    "        B, T, C = idx.shape # B = batch size, T = block size, C = n_embed\n",
    "        k = self.key(idx) # B, T, head_size\n",
    "        q = self.query(idx) # B, T, head_size\n",
    "        v = self.value(idx) # B, T, head_size\n",
    "        # Compute Attention scores. We need to multiply q with k but we can only do that if q is of shape (B, T, head_size) and k is of chape (B, head_size, T)\n",
    "        attention_scores = q @ k.transpose(-2, -1) * (C**(-0.5)) # (C**(-0.5)) is the normalization inv(sqrt(d_k)) in the origital attention paper\n",
    "        # attention_scores is of size (B, T, T)\n",
    "\n",
    "        # For decoder block, we need to do masked fill of attention scores since we don't want future to affect past. Fill upper triangle with -Inf before softmax\n",
    "        attention_scores = attention_scores.masked_fill(self.tril[:T,:T] == 0, float('-inf')) # (B, T, T)\n",
    "        softmax_scores = F.softmax(attention_scores, dim=-1)\n",
    "        softmax_scores_with_dropout = self.dropout(softmax_scores)\n",
    "        out = softmax_scores_with_dropout @ v # (B, T, T) @ (B, T, head_size) = (B, T, head_size)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(num_heads*head_size, n_embed)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, context):\n",
    "        out = torch.cat([h(context) for h in self.heads], dim = -1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class feedforward(nn.Module): # After multi heads, we need a feedforward network\n",
    "    def __init__(self, n_embed):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(n_embed, 4*n_embed), nn.ReLU(), nn.Linear(4*n_embed, n_embed), nn.Dropout(dropout))\n",
    "\n",
    "    def forward(self, idx):\n",
    "        return self.net(idx)\n",
    "\n",
    "# Now, all building blocks are complete. Let's put a block of transformer together\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, n_embed, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embed\n",
    "        self.multihead = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = feedforward(n_embed)\n",
    "        self.ln1 = nn.LayerNorm(n_embed)\n",
    "        self.ln2 = nn.LayerNorm(n_embed)\n",
    "\n",
    "    def forward(self, idx):\n",
    "        idx = self.multihead(self.ln1(idx))\n",
    "        idx = self.ffwd(self.ln2(idx))\n",
    "        return idx\n",
    "\n",
    "# Finally, we need to connect the data set into batches, blocks, vocab_size to the TransformerBlock\n",
    "class GPTModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
    "        self.TransformerBlocks = nn.Sequential(*[TransformerBlock(n_embed, n_head) for _ in range(n_layer)])\n",
    "        self.layernorm_final = nn.LayerNorm(n_embed)\n",
    "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
    "\n",
    "        self.apply(self._init_weights) # for better initialization\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, context, target = None):\n",
    "        B, T = context.shape\n",
    "        \n",
    "        tok_emb = self.token_embedding_table(context) # B, T, n_embed\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # T, n_embed\n",
    "        x = tok_emb + pos_emb # B, T, n_embed\n",
    "        x = self.TransformerBlocks(x)\n",
    "        x = self.layernorm_final(x)\n",
    "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
    "        if target is None:\n",
    "                loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            target = target.view(B*T)\n",
    "            loss = F.cross_entropy(logits, target)\n",
    "    \n",
    "        return logits, loss\n",
    "        \n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is the current context. It is of size (B, T)\n",
    "        # max_new_tokens is the max number of characters to generate\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, loss = self.forward(idx[:, -block_size:]) # We truncate idx to meet only block_size sized tensor array\n",
    "            relevant_logits = logits[:, -1, :] # we get the last time step in the sequence: becomes (B, 1, C).\n",
    "            probs = F.softmax(relevant_logits, dim = -1) # Becomes (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples = 1) # (B, 1)\n",
    "            idx = torch.cat((idx, idx_next), dim = 1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "a3b5748a-aa43-44a0-971b-3ee8c12930e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.406337 M parameters\n",
      "step 0: train loss 4.1873, val loss 4.1890, time taken = 3.625605821609497 seconds\n",
      "step 500: train loss 3.3215, val loss 3.3581, time taken = 3.7112720012664795 seconds\n",
      "step 1000: train loss 3.3155, val loss 3.3528, time taken = 3.477132797241211 seconds\n",
      "step 1500: train loss 3.3086, val loss 3.3451, time taken = 3.185731887817383 seconds\n",
      "step 2000: train loss 3.3004, val loss 3.3596, time taken = 3.3659539222717285 seconds\n",
      "step 2500: train loss 3.3075, val loss 3.3368, time taken = 3.3775699138641357 seconds\n",
      "step 3000: train loss 3.3119, val loss 3.3412, time taken = 3.198637008666992 seconds\n",
      "step 3500: train loss 3.3073, val loss 3.3518, time taken = 3.5656850337982178 seconds\n",
      "step 4000: train loss 3.3065, val loss 3.3503, time taken = 3.0073740482330322 seconds\n",
      "step 4500: train loss 3.3144, val loss 3.3466, time taken = 3.0669801235198975 seconds\n",
      "step 4999: train loss 3.3141, val loss 3.3520, time taken = 3.7007861137390137 seconds\n"
     ]
    }
   ],
   "source": [
    "model = GPTModel()\n",
    "m = model.to(device)\n",
    "print(\"device = \", device)\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    start = time.time()\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['test']:.4f}, time taken = {time.time()-start} seconds\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "out = m.generate(context, max_new_tokens=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "29e0f630-fafd-41ee-a7ae-7f09b9f13737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "wf\n",
      "fahereTtsEtneohc et  Thi edheetyitnt'rwi dsudp aiFtewhl uMe eroronrN UPiosrWnseo,f  Hoi  nmaedraaahnl AO rwThl ridd   vermeoeuliut ilt ,\n",
      "  eoa:ouu aKGdtdwd c m'irrtmdoeIal:bgly ro dnen   o dud ssoiKihorn, !rtdireprch  se   nr ep itlemedirevlgo ei  Toi f,elOy.o tOoiui I,ttthh\n",
      "uIidew a, rr tpvtugnpm  Lrsl:nff g  as eTUt:twt\n",
      "ntaawfnerm en U .brw rsf'Hll,er w  h.  u eincea fsG hoIemoatonoaidwh o itm\n",
      "oosenCsgyeroniehdaaewth yearbnicMwyvynnhI aaaiqNa.hlesmu ao eoee,o  Tnaaf \n",
      " mAhOdlwhtde\n",
      "soTctLt;aI\n"
     ]
    }
   ],
   "source": [
    "# n_layer = 4 # Number of transformer blocks to put in sequence\n",
    "# n_head = 4 # number of multi heads in each transformer block\n",
    "\n",
    "# # Parameters for each head are as follows:\n",
    "# batch_size = 16\n",
    "# block_size = 32\n",
    "# n_embed = 64\n",
    "# learning_rate = 3e-4\n",
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# max_iters = 100\n",
    "# max_iters = 5000\n",
    "# eval_interval = 500\n",
    "# eval_iters = 200\n",
    "\n",
    "# dropout = 0.0\n",
    "# head_size = n_embed\n",
    "print(decode(out[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a034970-04a9-40ec-96f5-45f0a7541350",
   "metadata": {},
   "source": [
    "# Let's try a bigger GPT model. Ideally, if you try the following model 10x times (5000 max_iters), you will start seeing some coherent language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a992d297-fc0a-4244-a0d5-f422ae5c7375",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059854c3-91c2-4b0e-b2d6-ef748cbf850f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_layer = 6 # Number of transformer blocks to put in sequence\n",
    "# n_head = 6 # number of multi heads in each transformer block\n",
    "\n",
    "# # Parameters for each head are as follows:\n",
    "# batch_size = 64\n",
    "# block_size = 256\n",
    "# n_embed = 384 # 64*6 = batch_size * n_head\n",
    "# learning_rate = 3e-4\n",
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# max_iters = 500\n",
    "# eval_interval = 10\n",
    "# eval_iters = 20\n",
    "\n",
    "# dropout = 0.2\n",
    "# head_size = n_embed\n",
    "\n",
    "model = GPTModel()\n",
    "m = model.to(device)\n",
    "print(\"device = \", device)\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    start = time.time()\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['test']:.4f}, time taken = {time.time()-start} seconds\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "out = m.generate(context, max_new_tokens=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "cbc0925c-4fda-4176-b26c-d7bdf4dd00b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ms\n",
      "otim:heisn\n",
      "dAevtinknhr:csi'hvrduRRolCuitut ow dhwt yhtreityi.mw oru\n",
      "hvie? tnr l a  rnnne hn h eseesm  p  mRtwabth b igaeT:enas iein gepitoobdaneddh hltH ,ertSuyslKsb nm  b onl y teod ynrs,seldd,or ;gtToeTosilyoe itsybmb aryNfAMddR : alhfcr rtph\n",
      "n\n",
      "artawdsrn\n",
      "  ariaIi\n",
      "vsaema ljw vWh , hodTohNkuyOiseYewChntrIi dtdhy ieado. eha\n",
      "odesl wueUttaeik,l s\n",
      " h dnyhS\n",
      "rl ere kd C vg\n",
      "veretEesfh ta  et .yaod lyeBUtem\n",
      "esA eytt:euya\n",
      "n,\n",
      "EuDsAnfolhE,arsEiL uw an.e; leensecedle :hduthd rmko eerAouEd arlB e\n",
      "j;t wy. \n"
     ]
    }
   ],
   "source": [
    "print(decode(out[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
